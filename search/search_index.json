{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"aws/","text":"AWS Lab In this lab, you'll learn : How to connect to AWS from your workstation. How to design an Infrastructure as Code (IaC) file. How to deploy this file in your Development Environment. Git Repo for IaC As mentionned before, we will maintain the IaC in a separated repo. Go ahead and create a new Git Repo as done before, this time named yncrea-cloudcomputing-iac . Info From now on, all the IaC files will be pushed to this repo. This way we can separate the automation of infra deployment and microservices deployments . Connect to AWS Educate Go to www.awseducate.com and log in with your Yncr\u00e9a E-Mail address. You should have already received an invitation. AWS Connection setup In this chapter, we'll explain you how to connect with the AWS CLI to your AWS Account. In the AWS Edcuate console, click on \"My Classrooms\". In the line of \"Cloud Computing\", click on \"Go to classroom\". Confirm with \"Continue\". In the new window, click on \"Account details\". In the pop-up window, click on the \"Show\" button next to \"AWS CLI:\". Copy and paste the content in the shown box to %HOME%/.aws/credentials . Open %HOME%/.aws/config and paste 1 2 3 [default] region = us-east-1 output = yaml Verify that you have access to the training account: 1 aws ec2 describe-instances Tip If you already have other AWS Profiles configured, name them in brackets : [default] aws_access_key_id = ... aws_secret_access_key = ... [other_profile] aws_access_key_id = ... aws_secret_access_key = ... And then add the --profile <profile_name> to each aws commands VPC A VPC means Virtual Private Cloud. This is what AWS provide us, and we will work inside our own VPC for the rest of this course. CloudFormation AWS CloudFormation is a YAML based language dedicated to IaC. It helps declaring ressources to define an architecture. Let's now deploy your first resource in the development account. Download the following file to your IaC Git Repo in src/infra folder. cloudformation.yaml This file deploys a single EC2 instance. Deploy the EC2 instance with the help of CloudFormation: 1 aws cloudformation deploy --template-file cloudformation.yaml --stack-name <your-login>-stack Wait until the stack is deployed and check if your instance is visible: 1 aws ec2 describe-instances Success Congratulation, you now have access to AWS and can create EC2 machines ! Follow up exercise Modify the CloudFormation template in order to add a Security Group which allows access on Port 443 to your EC2 instance. You can check the result in by verifying the new instance description in the EC2 view. Tip Don't forget to also checkout the EC2 CF Docs to know how to assigne the newly SG to the EC2 ! Info Note that AWS CloudFormation service won't recreate the full stack (if you use the same stack name ofc), but only add the necessary ressources. The feature is called Stack Update . It is also smart enough to restart the instance only if needed ! (Which is the case here) Troposphere Another way to generate CloudFormation templates is to use a framework for a programming language. One famous example is Troposphere , a Python Framework. Info The advantage of using a framework over bare CloudFormation is that you can use logic, conditions and loops when defining your infrastructure, so in short, it gives more flexibility when building it. In this example, we'll generate another EC2 instance. Download the following file to your \"CloudComputing\" folder: ec2_instance.py After you understood the logic of the file, let's deploy it now. Generate CloudFormation from the Python script: 1 python ec2_instance.py > ec2_instance.yml Warning You may need to install python depency : troposphere pip install troposphere Deploy the CloudFormation stack: 1 aws cloudformation deploy --template-file ec2_instance.yml --stack-name <your-ldap-login>-ts-stack Success Congratulation, you are able to deploy complex IaC ! Follow up exercise Start from the ec2_instance.py file created before and : Add the Security Group created before in CF, but this time using Troposphere. Package the EC2 Instance in a Launch Configuration that is referenced by an Auto Scaling Group. You can find an example here . Add the necessary ressources from the template above to ec2_instance.py . Generate the yaml from the TS Template ec2_instance.py . Deploy it. Verify that that your EC2 instance is backed by an Autoscaling group (have a look at the instance tags).","title":"AWS - Basics Lab"},{"location":"aws/#aws-lab","text":"In this lab, you'll learn : How to connect to AWS from your workstation. How to design an Infrastructure as Code (IaC) file. How to deploy this file in your Development Environment.","title":"AWS Lab"},{"location":"aws/#git-repo-for-iac","text":"As mentionned before, we will maintain the IaC in a separated repo. Go ahead and create a new Git Repo as done before, this time named yncrea-cloudcomputing-iac . Info From now on, all the IaC files will be pushed to this repo. This way we can separate the automation of infra deployment and microservices deployments .","title":"Git Repo for IaC"},{"location":"aws/#connect-to-aws-educate","text":"Go to www.awseducate.com and log in with your Yncr\u00e9a E-Mail address. You should have already received an invitation.","title":"Connect to AWS Educate"},{"location":"aws/#aws-connection-setup","text":"In this chapter, we'll explain you how to connect with the AWS CLI to your AWS Account. In the AWS Edcuate console, click on \"My Classrooms\". In the line of \"Cloud Computing\", click on \"Go to classroom\". Confirm with \"Continue\". In the new window, click on \"Account details\". In the pop-up window, click on the \"Show\" button next to \"AWS CLI:\". Copy and paste the content in the shown box to %HOME%/.aws/credentials . Open %HOME%/.aws/config and paste 1 2 3 [default] region = us-east-1 output = yaml Verify that you have access to the training account: 1 aws ec2 describe-instances Tip If you already have other AWS Profiles configured, name them in brackets : [default] aws_access_key_id = ... aws_secret_access_key = ... [other_profile] aws_access_key_id = ... aws_secret_access_key = ... And then add the --profile <profile_name> to each aws commands","title":"AWS Connection setup"},{"location":"aws/#vpc","text":"A VPC means Virtual Private Cloud. This is what AWS provide us, and we will work inside our own VPC for the rest of this course.","title":"VPC"},{"location":"aws/#cloudformation","text":"AWS CloudFormation is a YAML based language dedicated to IaC. It helps declaring ressources to define an architecture. Let's now deploy your first resource in the development account. Download the following file to your IaC Git Repo in src/infra folder. cloudformation.yaml This file deploys a single EC2 instance. Deploy the EC2 instance with the help of CloudFormation: 1 aws cloudformation deploy --template-file cloudformation.yaml --stack-name <your-login>-stack Wait until the stack is deployed and check if your instance is visible: 1 aws ec2 describe-instances Success Congratulation, you now have access to AWS and can create EC2 machines !","title":"CloudFormation"},{"location":"aws/#follow-up-exercise","text":"Modify the CloudFormation template in order to add a Security Group which allows access on Port 443 to your EC2 instance. You can check the result in by verifying the new instance description in the EC2 view. Tip Don't forget to also checkout the EC2 CF Docs to know how to assigne the newly SG to the EC2 ! Info Note that AWS CloudFormation service won't recreate the full stack (if you use the same stack name ofc), but only add the necessary ressources. The feature is called Stack Update . It is also smart enough to restart the instance only if needed ! (Which is the case here)","title":"Follow up exercise"},{"location":"aws/#troposphere","text":"Another way to generate CloudFormation templates is to use a framework for a programming language. One famous example is Troposphere , a Python Framework. Info The advantage of using a framework over bare CloudFormation is that you can use logic, conditions and loops when defining your infrastructure, so in short, it gives more flexibility when building it. In this example, we'll generate another EC2 instance. Download the following file to your \"CloudComputing\" folder: ec2_instance.py After you understood the logic of the file, let's deploy it now. Generate CloudFormation from the Python script: 1 python ec2_instance.py > ec2_instance.yml Warning You may need to install python depency : troposphere pip install troposphere Deploy the CloudFormation stack: 1 aws cloudformation deploy --template-file ec2_instance.yml --stack-name <your-ldap-login>-ts-stack Success Congratulation, you are able to deploy complex IaC !","title":"Troposphere"},{"location":"aws/#follow-up-exercise_1","text":"Start from the ec2_instance.py file created before and : Add the Security Group created before in CF, but this time using Troposphere. Package the EC2 Instance in a Launch Configuration that is referenced by an Auto Scaling Group. You can find an example here . Add the necessary ressources from the template above to ec2_instance.py . Generate the yaml from the TS Template ec2_instance.py . Deploy it. Verify that that your EC2 instance is backed by an Autoscaling group (have a look at the instance tags).","title":"Follow up exercise"},{"location":"cfn-nag/","text":"Cloudformation Linting As you saw in the presentation, all layers of your application must be properly secured. This what we call Defense in Depth . The lowest layer in a public cloud deployment is the infrastructure and in the case of AWS, we are deploying it through CloudFormation. There can be very common pitfalls when it comes to Infrastructure as Code, so it can be very advantageous to automate the scan for these. One very known tools is cfn-nag . Install Ruby: 1 choco install ruby Install cfn_nag : 1 gem install cfn-nag Scan the following file using cfn_nag_scan : cf-all.yaml Fix all issues that are reported through the scanner. There is already a Github Action available for this tool here . Integrate this lint program into your Github workflow and do the validation of your CF files before you actually deploy the resources in AWS. Success Congratulation, you are now able to validate your IaC before deploying it !","title":"AWS - CloudFormation Linting"},{"location":"cfn-nag/#cloudformation-linting","text":"As you saw in the presentation, all layers of your application must be properly secured. This what we call Defense in Depth . The lowest layer in a public cloud deployment is the infrastructure and in the case of AWS, we are deploying it through CloudFormation. There can be very common pitfalls when it comes to Infrastructure as Code, so it can be very advantageous to automate the scan for these. One very known tools is cfn-nag . Install Ruby: 1 choco install ruby Install cfn_nag : 1 gem install cfn-nag Scan the following file using cfn_nag_scan : cf-all.yaml Fix all issues that are reported through the scanner. There is already a Github Action available for this tool here . Integrate this lint program into your Github workflow and do the validation of your CF files before you actually deploy the resources in AWS. Success Congratulation, you are now able to validate your IaC before deploying it !","title":"Cloudformation Linting"},{"location":"cleanup/","text":"Cleanup Resources All the EC2, ECS, EKS etc ... ressources that your deployed are expensive ! Delete : The CloudFormation stacks for the EC2 and ECS : from aws cli or the web interface The EKS cluster : eksctl delete cluster <your_cluster_name>","title":"AWS - Cleanup"},{"location":"cleanup/#cleanup-resources","text":"All the EC2, ECS, EKS etc ... ressources that your deployed are expensive ! Delete : The CloudFormation stacks for the EC2 and ECS : from aws cli or the web interface The EKS cluster : eksctl delete cluster <your_cluster_name>","title":"Cleanup Resources"},{"location":"cloudformation/","text":"CloudFormation with Github Actions - CD This exercise is building up on AWS Lab and Github Actions Lab parts. The goal of this exercise is to automate the deployment of AWS resources once a change to the master branch of a repository occured. To achieve this: We are first going to work with the cloudformation.yaml file from previous lab. Note If you did followup exercise to create the EC2 instance backed with the AutoScaling group, you can use the generated yaml instead. Extend your Github Workflow to deploy your stack automatically. The most flexible way is to use the embedded aws cli inside the ubuntu image running our actions. aws cloudformation deploy --template-file src/infra/cloudformation.yaml --stack-name <your-login>-stack Tip Don't forget to configure the AWS CLI before. Success Congratulation, you are able to deploy an IaC automaticaly using Github Actions !","title":"AWS - CloudFormation with Github Actions / CD"},{"location":"cloudformation/#cloudformation-with-github-actions-cd","text":"This exercise is building up on AWS Lab and Github Actions Lab parts. The goal of this exercise is to automate the deployment of AWS resources once a change to the master branch of a repository occured. To achieve this: We are first going to work with the cloudformation.yaml file from previous lab. Note If you did followup exercise to create the EC2 instance backed with the AutoScaling group, you can use the generated yaml instead. Extend your Github Workflow to deploy your stack automatically. The most flexible way is to use the embedded aws cli inside the ubuntu image running our actions. aws cloudformation deploy --template-file src/infra/cloudformation.yaml --stack-name <your-login>-stack Tip Don't forget to configure the AWS CLI before. Success Congratulation, you are able to deploy an IaC automaticaly using Github Actions !","title":"CloudFormation with Github Actions - CD"},{"location":"docker/","text":"What about Docker ? You may know that Docker is about Containers, so let's start by answering the question : What is a Docker Container ? by visiting the Docker site . Building & Deploying manually with Docker The goal here is to Containerize the Spring Boot Microservice we just created. Then, we are 100% sure that running that way, it will be fine on ANY system ! Tip The best way to avoid the famous \"But, it is working on my machine !!\" Create the Dockerfile Create the file named Dockerfile in the root directory of your project, containing : 1 2 3 4 5 FROM java:8 EXPOSE 8080 ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-jar\" , \"/app.jar\" ] Info Line 1 : Our micro service is already embedding Tomcat. All it needs to run is Java, so the image can be based on Java only. Line 2 : We want to expose the port 8080 of tomcat to acces our APIs from outside the container. Line 3-5 : They are just about putting the build jar to the root of tomcat server, and run it. The entrypoint is usually a bash script, or as here, a simple bash command. Build the micro service using maven to get the JAR file ready int the target repository: mvn clean package Build the docker image manually docker build -t <your_docker_id>/yncrea-hellomicro:latest . It will generate the docker image for the micro service and make it available in the local docker registry. The image is tagged as following : docker images REPOSITORY TAG IMAGE ID CREATED SIZE <your_docker_id>/yncrea-hellomicro latest b13bcef528c5 46 minutes ago 681MB Run the image using Docker CLI : docker run -p 8080 :8080 -t <your_docker_id>/yncrea-hellomicro:latest Access your application REST APIs with your browser. Example : http://minikube-ip:8080/hello or http://localhost:8080/hello when using Docker Desktop. Tip To get the IP of your minikube cluster run the command : minikube ip You could also set this ip in your hosts file with the name minikube-ip ... Building automatically with Maven Automatize the build using Maven Fabric8 plugin : Find the latest Fabric8 plugin online Add the dependency to you pom. Solution <dependency> <groupId> io.fabric8 </groupId> <artifactId> docker-maven-plugin </artifactId> <version> 0.33.0 </version> </dependency> Configure the plugin to build, tag, and push the image to the local docker registry. Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <build> <plugins> ... <plugin> <groupId> io.fabric8 </groupId> <artifactId> docker-maven-plugin </artifactId> <configuration> <images> <image> <name> [your_docker_id]/yncrea-hellomicro </name> </image> </images> </configuration> <executions> <execution> <id> docker-build </id> <phase> pre-integration-test </phase> <goals> <goal> build </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Tip You can even use the \"save\" goal of the plugin in a new execution , in order to save the image as a tar file locally. All this, during the maven \"pre-integration-test\" phase ! Verify . Check your newly built image has been pushed to your local registry (by checking the 'created' time): docker images REPOSITORY TAG IMAGE ID CREATED SIZE <your_docker_id>/yncrea-hellomicro latest b13bcef528c5 10 seconds ago 681MB Cleanup Now it is time to delete our docker container otherwise we will pollute the next labs. List all docker container running docker ps Find the ID of the Docker Container you just created and use the first 3 digits of it to stop it : docker stop XYZ You should not see any containers running yncrea-hellomicro image when doing docker ps . Access your application REST APIs with your browser should NOT WORK anymore.","title":"Docker - Containers Lab"},{"location":"docker/#what-about-docker","text":"You may know that Docker is about Containers, so let's start by answering the question : What is a Docker Container ? by visiting the Docker site .","title":"What about Docker ?"},{"location":"docker/#building-deploying-manually-with-docker","text":"The goal here is to Containerize the Spring Boot Microservice we just created. Then, we are 100% sure that running that way, it will be fine on ANY system ! Tip The best way to avoid the famous \"But, it is working on my machine !!\" Create the Dockerfile Create the file named Dockerfile in the root directory of your project, containing : 1 2 3 4 5 FROM java:8 EXPOSE 8080 ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-jar\" , \"/app.jar\" ] Info Line 1 : Our micro service is already embedding Tomcat. All it needs to run is Java, so the image can be based on Java only. Line 2 : We want to expose the port 8080 of tomcat to acces our APIs from outside the container. Line 3-5 : They are just about putting the build jar to the root of tomcat server, and run it. The entrypoint is usually a bash script, or as here, a simple bash command. Build the micro service using maven to get the JAR file ready int the target repository: mvn clean package Build the docker image manually docker build -t <your_docker_id>/yncrea-hellomicro:latest . It will generate the docker image for the micro service and make it available in the local docker registry. The image is tagged as following : docker images REPOSITORY TAG IMAGE ID CREATED SIZE <your_docker_id>/yncrea-hellomicro latest b13bcef528c5 46 minutes ago 681MB Run the image using Docker CLI : docker run -p 8080 :8080 -t <your_docker_id>/yncrea-hellomicro:latest Access your application REST APIs with your browser. Example : http://minikube-ip:8080/hello or http://localhost:8080/hello when using Docker Desktop. Tip To get the IP of your minikube cluster run the command : minikube ip You could also set this ip in your hosts file with the name minikube-ip ...","title":"Building &amp; Deploying manually with Docker"},{"location":"docker/#building-automatically-with-maven","text":"Automatize the build using Maven Fabric8 plugin : Find the latest Fabric8 plugin online Add the dependency to you pom. Solution <dependency> <groupId> io.fabric8 </groupId> <artifactId> docker-maven-plugin </artifactId> <version> 0.33.0 </version> </dependency> Configure the plugin to build, tag, and push the image to the local docker registry. Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <build> <plugins> ... <plugin> <groupId> io.fabric8 </groupId> <artifactId> docker-maven-plugin </artifactId> <configuration> <images> <image> <name> [your_docker_id]/yncrea-hellomicro </name> </image> </images> </configuration> <executions> <execution> <id> docker-build </id> <phase> pre-integration-test </phase> <goals> <goal> build </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Tip You can even use the \"save\" goal of the plugin in a new execution , in order to save the image as a tar file locally. All this, during the maven \"pre-integration-test\" phase ! Verify . Check your newly built image has been pushed to your local registry (by checking the 'created' time): docker images REPOSITORY TAG IMAGE ID CREATED SIZE <your_docker_id>/yncrea-hellomicro latest b13bcef528c5 10 seconds ago 681MB","title":"Building automatically with Maven"},{"location":"docker/#cleanup","text":"Now it is time to delete our docker container otherwise we will pollute the next labs. List all docker container running docker ps Find the ID of the Docker Container you just created and use the first 3 digits of it to stop it : docker stop XYZ You should not see any containers running yncrea-hellomicro image when doing docker ps . Access your application REST APIs with your browser should NOT WORK anymore.","title":"Cleanup"},{"location":"ecs/","text":"ECS Lab The goal of this exercise is to deploy your previously built Docker image automatically to the AWS Elastic Container Service : ECS . This diagram shows how ECS works : First, deploy an ECS cluster with CloudFormation from the following template (and add it to your repo): ecs-cluster.yaml Choose the default Security Group, VPC and 2 Subnets. Tip You can find these info in the AWS console : EC2 & VPC parts. Parameters can be passed to the CF Stack using the --parameter-overrides option. You may also need to use --capabilities CAPABILITY_NAMED_IAM in order to allow the stack to create the roles. Add the following file to your repository. Try to understand each part of it. ecs-task.yaml Use the TaskDefinition above to deploy your docker image in ECS 1 aws cloudformation deploy --template-file ecs-task.yaml --stack-name ecs-task --parameter-overrides ImageUrl=docker.io/<your-docker-id>/yncrea-hellomicro:sha-$(git rev-parse --short HEAD) --no-fail-on-empty-changeset Use a web browser and try to access the EC2 running the container public URI on port 8080 ! Tip You can find the URI by browsing the AWS Console to EC2 Service > Select you instance > Public DNS (IPv4) Success Congratulation, you deployed your Docker image in ECS and can access your API through the Web ! Next step for Production would be to have a LoadBalancer in front of this ... Extend your Github flow accordingly. Success Congratulation, you are now able to Continuously Deliver Docker images ! Followup Exercise Checkout the two links below and modifiy your ECS cluster to be backed up by a Public Load Balancer & Target Group : Cluster with LB Service Verify all is working fine by going to you Public Load Balancer URL (Public DNS Name) on port 8080. Warning Be careful to update the templates to uses your existing VPC, Subnets, Security Groups etc ... instead of creating new ones as they do in the template ! Also, you need to add a LoadBalancer Listener & Rule on port 8080 to redirect calls to our micro service to the right Target Group .","title":"AWS - ECS Lab"},{"location":"ecs/#ecs-lab","text":"The goal of this exercise is to deploy your previously built Docker image automatically to the AWS Elastic Container Service : ECS . This diagram shows how ECS works : First, deploy an ECS cluster with CloudFormation from the following template (and add it to your repo): ecs-cluster.yaml Choose the default Security Group, VPC and 2 Subnets. Tip You can find these info in the AWS console : EC2 & VPC parts. Parameters can be passed to the CF Stack using the --parameter-overrides option. You may also need to use --capabilities CAPABILITY_NAMED_IAM in order to allow the stack to create the roles. Add the following file to your repository. Try to understand each part of it. ecs-task.yaml Use the TaskDefinition above to deploy your docker image in ECS 1 aws cloudformation deploy --template-file ecs-task.yaml --stack-name ecs-task --parameter-overrides ImageUrl=docker.io/<your-docker-id>/yncrea-hellomicro:sha-$(git rev-parse --short HEAD) --no-fail-on-empty-changeset Use a web browser and try to access the EC2 running the container public URI on port 8080 ! Tip You can find the URI by browsing the AWS Console to EC2 Service > Select you instance > Public DNS (IPv4) Success Congratulation, you deployed your Docker image in ECS and can access your API through the Web ! Next step for Production would be to have a LoadBalancer in front of this ... Extend your Github flow accordingly. Success Congratulation, you are now able to Continuously Deliver Docker images !","title":"ECS Lab"},{"location":"ecs/#followup-exercise","text":"Checkout the two links below and modifiy your ECS cluster to be backed up by a Public Load Balancer & Target Group : Cluster with LB Service Verify all is working fine by going to you Public Load Balancer URL (Public DNS Name) on port 8080. Warning Be careful to update the templates to uses your existing VPC, Subnets, Security Groups etc ... instead of creating new ones as they do in the template ! Also, you need to add a LoadBalancer Listener & Rule on port 8080 to redirect calls to our micro service to the right Target Group .","title":"Followup Exercise"},{"location":"eks/","text":"EKS Lab You have already experimented with a local Kubernetes installation in the form of Minikube. In the next step, we will set up a managed Kubernetes on AWS. For this, we will use EKS , the Elastic Kubernetes Service . This diagram shows how EKS works : EKS Cluster Deployment To facilitate the instantiation of Kubernetes, we will use eksctl , a command line tool interact with Kubernetes. To install it, please follow the guidelines here . After the installation, please download the following file: eks-cluster.yaml Danger It is absolutely necessary that you use the file above for cluster definition. The AWS Educate has certain limitations that makes the \"standard\" installation fail. You can then instantiate an EKS cluster by simply executing the following command: eksctl create cluster -f src/infra/eks-cluster.yaml Tip You can use --profile same as in aws commands to specify a specific profile if you have several AWS Accounts. Configure Kubectl Amazon EKS uses the aws eks get-token command, available in version 1.18.49 or later of the AWS CLI or the AWS IAM Authenticator for Kubernetes with kubectl for cluster authentication. If you have installed the AWS CLI on your system, then by default the AWS IAM Authenticator for Kubernetes will use the same credentials that are returned with the following command: aws sts get-caller-identity Ensure that you have version 1.18.49 or later of the AWS CLI installed. To install or upgrade the AWS CLI, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide . Warning Your system's Python version must be 2.7.9 or later. Otherwise, you receive hostname doesn't match errors with AWS CLI calls to Amazon EKS. You can check your AWS CLI version with the following command: aws --version Use the AWS CLI update-kubeconfig command to create or update your kubeconfig for your cluster. By default, the resulting configuration file is created at the default kubeconfig path .kube/config <code>.kube/config</code> in your home directory or merged with an existing kubeconfig at that location. You can specify another path with the --kubeconfig option. aws eks --region <region-code> update-kubeconfig --name <cluster_name> Test your configuration. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 1m Tip If you receive the error \"aws-iam-authenticator\": executable file not found in $PATH , your kubectl isn't configured for Amazon EKS. For more information, see Installing aws-iam-authenticator . (More details & troubleshooting here ) Chart deployment Install Helm backend service Tiller service account (Tiller is the Helm server-side component) : kubectl create serviceaccount tiller --namespace kube-system Create the service-account.yaml file in your repo apiVersion : v1 kind : ServiceAccount metadata : name : tiller namespace : kube-system Create the rbac-config.yaml file in your repo kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system Run apply commands and helm init kubectl apply -f src/infra/service-account.yaml kubectl apply -f src/infra/rbac-config.yaml helm init --service-account tiller We can see tiller-deploy is a running pod in kube-system namespace : kubectl get pods --namespace kube-system | grep tiller tiller-deploy-758bcdc94f-4c77s 0 /1 Pending 0 71s kubectl get pods --namespace kube-system | grep tiller tiller-deploy-758bcdc94f-4c77s 0 /1 Running 0 92s Deploy our Chart Deploy the chart from previous Labs : helm install --name silly-unicorn src/helm/chart/yncrea-hellomicro Note Helm CLI is using the same config file as Kubectl, nothing to change here to target EKS ! Check running pods / services : kubectl.exe get pods NAME READY STATUS RESTARTS AGE yncrea-hellomicro-646cb8c96-5g7td 1 /1 Running 0 kubectl.exe get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 15m yncrea-hellomicro-service NodePort 10 .100.136.153 <none> 80 :30080/TCP 51s Add an Inbound Rule on the Security Group eks-nodegroup to allow TCP traffic from all IPs to your worker node on port 30080 . Connect to your worker node EC2 instance public DNS Name on port 30080 and check your micro service is running fine. Success Congratulations ! You are able to deploy Chart in EKS ! Github Action After the cluster installation, it's time to extend your Github Actions workflow to launch the deployment in Kubernetes. You'll need to extend your workflow with the following lines: 1 2 3 aws eks -- region < region - code > update - kubeconfig -- name < cluster_name > helm upgrade -- install silly - unicorn src / helm / chart / yncrea - hellomicro Note We won't set up the EKS cluster creation in Github Actions as it is heavy and not useful to deploy / update at each commit. Success Congratulations ! At this point you are able to build and test Docker Images (CI) and use them in a Helm Chart, automatically deployed on AWS EKS (CD) ! Followup Exercise Create a CF file to add the missing Application Load Balancer to your Infra, in order to access your micro service by a fixed DNS Name. Analyze the file cf-all.yaml used in the Cloud Formation Linting chapter. Once you understand everything, deploy it to have your first app example using a Database !","title":"AWS - EKS Lab"},{"location":"eks/#eks-lab","text":"You have already experimented with a local Kubernetes installation in the form of Minikube. In the next step, we will set up a managed Kubernetes on AWS. For this, we will use EKS , the Elastic Kubernetes Service . This diagram shows how EKS works :","title":"EKS Lab"},{"location":"eks/#eks-cluster-deployment","text":"To facilitate the instantiation of Kubernetes, we will use eksctl , a command line tool interact with Kubernetes. To install it, please follow the guidelines here . After the installation, please download the following file: eks-cluster.yaml Danger It is absolutely necessary that you use the file above for cluster definition. The AWS Educate has certain limitations that makes the \"standard\" installation fail. You can then instantiate an EKS cluster by simply executing the following command: eksctl create cluster -f src/infra/eks-cluster.yaml Tip You can use --profile same as in aws commands to specify a specific profile if you have several AWS Accounts.","title":"EKS Cluster Deployment"},{"location":"eks/#configure-kubectl","text":"Amazon EKS uses the aws eks get-token command, available in version 1.18.49 or later of the AWS CLI or the AWS IAM Authenticator for Kubernetes with kubectl for cluster authentication. If you have installed the AWS CLI on your system, then by default the AWS IAM Authenticator for Kubernetes will use the same credentials that are returned with the following command: aws sts get-caller-identity Ensure that you have version 1.18.49 or later of the AWS CLI installed. To install or upgrade the AWS CLI, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide . Warning Your system's Python version must be 2.7.9 or later. Otherwise, you receive hostname doesn't match errors with AWS CLI calls to Amazon EKS. You can check your AWS CLI version with the following command: aws --version Use the AWS CLI update-kubeconfig command to create or update your kubeconfig for your cluster. By default, the resulting configuration file is created at the default kubeconfig path .kube/config <code>.kube/config</code> in your home directory or merged with an existing kubeconfig at that location. You can specify another path with the --kubeconfig option. aws eks --region <region-code> update-kubeconfig --name <cluster_name> Test your configuration. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 1m Tip If you receive the error \"aws-iam-authenticator\": executable file not found in $PATH , your kubectl isn't configured for Amazon EKS. For more information, see Installing aws-iam-authenticator . (More details & troubleshooting here )","title":"Configure Kubectl"},{"location":"eks/#chart-deployment","text":"Install Helm backend service Tiller service account (Tiller is the Helm server-side component) : kubectl create serviceaccount tiller --namespace kube-system Create the service-account.yaml file in your repo apiVersion : v1 kind : ServiceAccount metadata : name : tiller namespace : kube-system Create the rbac-config.yaml file in your repo kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system Run apply commands and helm init kubectl apply -f src/infra/service-account.yaml kubectl apply -f src/infra/rbac-config.yaml helm init --service-account tiller We can see tiller-deploy is a running pod in kube-system namespace : kubectl get pods --namespace kube-system | grep tiller tiller-deploy-758bcdc94f-4c77s 0 /1 Pending 0 71s kubectl get pods --namespace kube-system | grep tiller tiller-deploy-758bcdc94f-4c77s 0 /1 Running 0 92s Deploy our Chart Deploy the chart from previous Labs : helm install --name silly-unicorn src/helm/chart/yncrea-hellomicro Note Helm CLI is using the same config file as Kubectl, nothing to change here to target EKS ! Check running pods / services : kubectl.exe get pods NAME READY STATUS RESTARTS AGE yncrea-hellomicro-646cb8c96-5g7td 1 /1 Running 0 kubectl.exe get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 15m yncrea-hellomicro-service NodePort 10 .100.136.153 <none> 80 :30080/TCP 51s Add an Inbound Rule on the Security Group eks-nodegroup to allow TCP traffic from all IPs to your worker node on port 30080 . Connect to your worker node EC2 instance public DNS Name on port 30080 and check your micro service is running fine. Success Congratulations ! You are able to deploy Chart in EKS !","title":"Chart deployment"},{"location":"eks/#github-action","text":"After the cluster installation, it's time to extend your Github Actions workflow to launch the deployment in Kubernetes. You'll need to extend your workflow with the following lines: 1 2 3 aws eks -- region < region - code > update - kubeconfig -- name < cluster_name > helm upgrade -- install silly - unicorn src / helm / chart / yncrea - hellomicro Note We won't set up the EKS cluster creation in Github Actions as it is heavy and not useful to deploy / update at each commit. Success Congratulations ! At this point you are able to build and test Docker Images (CI) and use them in a Helm Chart, automatically deployed on AWS EKS (CD) !","title":"Github Action"},{"location":"eks/#followup-exercise","text":"Create a CF file to add the missing Application Load Balancer to your Infra, in order to access your micro service by a fixed DNS Name. Analyze the file cf-all.yaml used in the Cloud Formation Linting chapter. Once you understand everything, deploy it to have your first app example using a Database !","title":"Followup Exercise"},{"location":"further/","text":"Going Further Here are some interesting topics to go deeper in the DevOps culture : Deploy your Infra as Code using Terraform Checkout Ansible Use AWS Service Catalog to have prod ready products ...","title":"Ideas"},{"location":"further/#going-further","text":"Here are some interesting topics to go deeper in the DevOps culture : Deploy your Infra as Code using Terraform Checkout Ansible Use AWS Service Catalog to have prod ready products ...","title":"Going Further"},{"location":"github-actions/","text":"Github Actions Lab - CI The goal of this exercise is to show you how to create a CI/CD Pipeline for containerized applications. For this Lab, we will use the automation platform of Github, called Github Actions . At the end of the exercise, you should have added a Github Actions YAML file to your repository and on each commit and push to the master branch, a new Docker image should be built automatically, and pushed to dockerhub. Danger Performing this lab is absolutely necessary for most of the following exercises. CI / CD ? We always refer to the automatization of code source build and deployment with the terms CI/CD, for Continuous Integration and Continuous Deployment. Several tools are able to help us automatize all the steps from the source code to the project running in production. Warning Continuous Integration is automating the Build steps and the Testing part, to obtain a validated delivrable package. CD can mean : Continuous Delivery or Continuous Deployment . The main difference is that Delivery means : we do not push to prod automatically. Here is a good sum up made by AWS CI/CD teams Basic Action First, let's create a basic action, doing a simple Echo from Mona, the Octocat. Create the .github/workflows directory needed to use github actions. Create the file build.yaml in here. Compile, Test & Package the Code Since we already did the job in maven, the easiest is to reuse this in an ubuntu based action with Java enabled, and some mvn steps . Update the build.yaml file : make it clean, give proper names to steps, remove the basic step ... Tip mvn is embededd and configured in ubuntu:latest github-action container. Yet, java need to be configured : ... - name : Set up JDK 1.8 uses : actions/setup-java@v1 with : java-version : 1.8 ... Warning Don't forget to checkout the code of your \u00b5S inside the github action container at the begining Also, choose the most relevant trigger : On push ? On pull request ? On which branch ? Build & Push the Image Let's simply use an existing action provided by DockerHub . Login to dockerhub and create a repository for your image : <your_docker_id>/cloudcomputing Update the file build.yaml again. Use the Github Action provided by Docker ( github link ) Push your image with a unique tag to hub.docker.com (use the GITHUB_SHA environment variable) Warning Don't forget to create your DockerHub credentials as Github Secrets in your repo settings !!! Check your image has been pushed to your DockerHub Account Registry. Success Congratulation, your Docker image is ready to be used by anybody ! Followup Exercise Tip You can setup a build notification on #Slack ! voxmedia/github-action-slack-notify-build . Scopes used are : chat:write channels:read groups:read Tip You can setup an online Documentation using Github Pages ! Deploy MkDocs .","title":"Github - Actions Lab / CI"},{"location":"github-actions/#github-actions-lab-ci","text":"The goal of this exercise is to show you how to create a CI/CD Pipeline for containerized applications. For this Lab, we will use the automation platform of Github, called Github Actions . At the end of the exercise, you should have added a Github Actions YAML file to your repository and on each commit and push to the master branch, a new Docker image should be built automatically, and pushed to dockerhub. Danger Performing this lab is absolutely necessary for most of the following exercises.","title":"Github Actions Lab - CI"},{"location":"github-actions/#ci-cd","text":"We always refer to the automatization of code source build and deployment with the terms CI/CD, for Continuous Integration and Continuous Deployment. Several tools are able to help us automatize all the steps from the source code to the project running in production. Warning Continuous Integration is automating the Build steps and the Testing part, to obtain a validated delivrable package. CD can mean : Continuous Delivery or Continuous Deployment . The main difference is that Delivery means : we do not push to prod automatically. Here is a good sum up made by AWS CI/CD teams","title":"CI / CD ?"},{"location":"github-actions/#basic-action","text":"First, let's create a basic action, doing a simple Echo from Mona, the Octocat. Create the .github/workflows directory needed to use github actions. Create the file build.yaml in here.","title":"Basic Action"},{"location":"github-actions/#compile-test-package-the-code","text":"Since we already did the job in maven, the easiest is to reuse this in an ubuntu based action with Java enabled, and some mvn steps . Update the build.yaml file : make it clean, give proper names to steps, remove the basic step ... Tip mvn is embededd and configured in ubuntu:latest github-action container. Yet, java need to be configured : ... - name : Set up JDK 1.8 uses : actions/setup-java@v1 with : java-version : 1.8 ... Warning Don't forget to checkout the code of your \u00b5S inside the github action container at the begining Also, choose the most relevant trigger : On push ? On pull request ? On which branch ?","title":"Compile, Test &amp; Package the Code"},{"location":"github-actions/#build-push-the-image","text":"Let's simply use an existing action provided by DockerHub . Login to dockerhub and create a repository for your image : <your_docker_id>/cloudcomputing Update the file build.yaml again. Use the Github Action provided by Docker ( github link ) Push your image with a unique tag to hub.docker.com (use the GITHUB_SHA environment variable) Warning Don't forget to create your DockerHub credentials as Github Secrets in your repo settings !!! Check your image has been pushed to your DockerHub Account Registry. Success Congratulation, your Docker image is ready to be used by anybody !","title":"Build &amp; Push the Image"},{"location":"github-actions/#followup-exercise","text":"Tip You can setup a build notification on #Slack ! voxmedia/github-action-slack-notify-build . Scopes used are : chat:write channels:read groups:read Tip You can setup an online Documentation using Github Pages ! Deploy MkDocs .","title":"Followup Exercise"},{"location":"github/","text":"Getting started with Github The goal of this exercise is to upload your current progress into a version control system . For this course, we are using Github as our platform. Repository Sign Up to Github.com Install git on your workstation (with any package manager or from their website ) From the Github web interface, create a new repository with the name yncrea-cloudcomputing-microservice . It will serve as the base for the following days and all changes will be stored here. Follow the instructions on how to add existing source code into your newly created repository. Perform the indicated steps at the root of your project folder. Info A good practice is to separate the micro services and the infra . From now on, each micro service will have its own git repository. With the same logic, the infra as code will also have its own repo. Push your source code into your repository Info Github Guides is a good source of basic knowledge. Check it out :) Once this is done, you can take the remaining time and get familiar with the Gitflow best practice here . Info Ideally, for future labs you will create a new branch for every exercise and merge it to the Git master through a \"Pull Request\". Go ahead and prepare your first branch for the next exercise. Success Congratulation, you have your own Github repository, and you are part of the Open Source community !","title":"Github - Git Repository Lab"},{"location":"github/#getting-started-with-github","text":"The goal of this exercise is to upload your current progress into a version control system . For this course, we are using Github as our platform.","title":"Getting started with Github"},{"location":"github/#repository","text":"Sign Up to Github.com Install git on your workstation (with any package manager or from their website ) From the Github web interface, create a new repository with the name yncrea-cloudcomputing-microservice . It will serve as the base for the following days and all changes will be stored here. Follow the instructions on how to add existing source code into your newly created repository. Perform the indicated steps at the root of your project folder. Info A good practice is to separate the micro services and the infra . From now on, each micro service will have its own git repository. With the same logic, the infra as code will also have its own repo. Push your source code into your repository Info Github Guides is a good source of basic knowledge. Check it out :) Once this is done, you can take the remaining time and get familiar with the Gitflow best practice here . Info Ideally, for future labs you will create a new branch for every exercise and merge it to the Git master through a \"Pull Request\". Go ahead and prepare your first branch for the next exercise. Success Congratulation, you have your own Github repository, and you are part of the Open Source community !","title":"Repository"},{"location":"javaspringboot/","text":"SpringBoot Lab The goal here is to create a sample micro service coded in Java Spring Boot built with Maven. Create the micro service using Spring Initializr online tool. Warning Don't forget to add \"Spring Web\" dependency ! Click on \"Generate\" to download the zipped sources. Extract them in your work directory. Import it in your favorite Java IDE as a Maven project. Tip IntelliJ Idea Community Edition is a very good choice ... Just saying ! Tip If you don't use IntelliJ and need to install Maven, here is the command 1 choco install maven Edit the code Modify the main class : src/main/java/com/yncrea/cloudcomputing/CloudcomputingApplication.java as below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package com.yncrea.cloudcomputing ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.ResponseBody ; import org.springframework.web.bind.annotation.RestController ; @RestController @SpringBootApplication public class CloudcomputingApplication { @RequestMapping ( \"/\" ) public String home () { String hostname = System . getenv ( \"HOSTNAME\" ); return \"Hello Docker World - hostname : \" + hostname ; } @RequestMapping ( \"/hello\" ) @ResponseBody public String sayHello () { return \"Hello \" + System . getenv ( \"GREETING\" ); } @RequestMapping ( \"/secret\" ) @ResponseBody public String getSecret () { String secret = System . getenv ( \"MY_SECRET\" ) == null ? \"\" : System . getenv ( \"MY_SECRET\" ); return \"Secret : \" + secret ; } public static void main ( String [] args ) { SpringApplication . run ( CloudcomputingApplication . class , args ); } } Test it with the embedded Tomcat Server Build the project using your IDE Maven Window or Using the command line : 1 mvn clean install Run the application using your IDE Run Configuration or Using the command line : 1 mvn spring-boot:run The application is now accessible at http://localhost:8080 Tip Try visiting http://localhost:8080/hello to reach your 2 nd API","title":"Code - SpringBoot Lab"},{"location":"javaspringboot/#springboot-lab","text":"The goal here is to create a sample micro service coded in Java Spring Boot built with Maven. Create the micro service using Spring Initializr online tool. Warning Don't forget to add \"Spring Web\" dependency ! Click on \"Generate\" to download the zipped sources. Extract them in your work directory. Import it in your favorite Java IDE as a Maven project. Tip IntelliJ Idea Community Edition is a very good choice ... Just saying ! Tip If you don't use IntelliJ and need to install Maven, here is the command 1 choco install maven Edit the code Modify the main class : src/main/java/com/yncrea/cloudcomputing/CloudcomputingApplication.java as below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package com.yncrea.cloudcomputing ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.ResponseBody ; import org.springframework.web.bind.annotation.RestController ; @RestController @SpringBootApplication public class CloudcomputingApplication { @RequestMapping ( \"/\" ) public String home () { String hostname = System . getenv ( \"HOSTNAME\" ); return \"Hello Docker World - hostname : \" + hostname ; } @RequestMapping ( \"/hello\" ) @ResponseBody public String sayHello () { return \"Hello \" + System . getenv ( \"GREETING\" ); } @RequestMapping ( \"/secret\" ) @ResponseBody public String getSecret () { String secret = System . getenv ( \"MY_SECRET\" ) == null ? \"\" : System . getenv ( \"MY_SECRET\" ); return \"Secret : \" + secret ; } public static void main ( String [] args ) { SpringApplication . run ( CloudcomputingApplication . class , args ); } } Test it with the embedded Tomcat Server Build the project using your IDE Maven Window or Using the command line : 1 mvn clean install Run the application using your IDE Run Configuration or Using the command line : 1 mvn spring-boot:run The application is now accessible at http://localhost:8080 Tip Try visiting http://localhost:8080/hello to reach your 2 nd API","title":"SpringBoot Lab"},{"location":"kubernetes/","text":"Kubernetes Lab We saw that Docker helps creating Containers. Now we need to manage these containers : here comes Kubernetes, the \"Container Orchestrator\" , also called \"K8S\" . Please visit their website explaining in details what is K8S ... In this lab, we will use Minikube as a local kubernetes, for develoment only. Info No matter if you used Gokube for previous Labs, it is running Minikube behind the scenes If you used Docker Desktop you don't have Minikube but an embedded K8S Cluster working the same way, all good ! Simple Application Deployment Verify that you can access Kubernetes: 1 kubectl version If you see a Server Version like below, it means your Kubernetes CLI can connect to your Kubernetes VM: 1 2 3 $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.10\" , GitCommit: \"37d169313237cb4ceb2cc4bef300f2ae3053c1a2\" , GitTreeState: \"clean\" , BuildDate: \"2019-08-19T10:52:43Z\" , GoVersion: \"go1.11.13\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.10\" , GitCommit: \"37d169313237cb4ceb2cc4bef300f2ae3053c1a2\" , GitTreeState: \"clean\" , BuildDate: \"2019-08-19T10:44:49Z\" , GoVersion: \"go1.11.13\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Create the folder src/kubernetes in your project. Download the following file to your the newly created folder: deployment.yaml Open the file in an editor and verify that the image: key is referencing your previously built image. Deploy your application with the following command: 1 kubectl apply -f src/kubernetes/deployment.yaml Verify that your application is running properly: 1 kubectl get deployment You shoud now see one running Pod , which is scheduled by the Deployment that you just created. Tip The number of pods (more specifically, the number of containers) running is displayed in the \"READY\" column. 1/1 means 1 container is working fine out of 1 expected, all good ! Info Even if you have one pod running, if now try to access your application REST APIs through your browser, it should NOT work. (Unless you forgot to delete the docker container from previous Lab, in this case, the old container is repsonding, not the newly created pod !) You can also check the running Pods in your Kubernetes cluster by typing: 1 kubectl get pods This will give a list of running instances (a.k.a. Pods) of your application. Tip Write down the name of the Pod, you'll need it later for reference. In order to access your application, you have to deploy a Kubernetes Service . Download the following file to your kubernetes folder: service.yaml and apply the following command: 1 kubectl apply -f src/kubernetes/service.yaml You have now deployed a so called NodePort Kubernetes Service . It opens a dedicated port on your Minikube VM, through which you can access the according service. You can find the associated port number by typing: 1 kubectl get svc In the example below, the port number would be 30080 : 1 2 3 4 5 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 14d yncrea-hellomicro-service NodePort 10 .103.108.153 <none> 80 :30080/TCP 2m15s Info If not set in the service, Kubernetes would allow a random NodePort between 30000 and 32767. In your browser, open the IP of your Minikube VM (which you retrieved in the docker lab) and add the port that you retrieved from the last command, e.g.: http://minikube-ip:30080 . Tip When using Docker Desktop embedded K8S in place of minikube, you can directly hit on http://localhost:30080 ! Success You should see \"Hello World\" example from before, but it's hosted in Kubernetes . In addition you will notice that the micro service is now displaying the hostname . Indeed, now the container is orchestrated by kubernetes, and kubernetes filled the default env vars such as HOSTNAME used in our microservice Java Code ;) Application Scaling Now you'll see the scaling capabilities of Kubernetes. Enter the following command: 1 kubectl.exe scale deployment/yncrea-hellomicro --replicas=3 With this command, you update the Kubernetes Deployment and instruct it to have a total of three replicas. Kubernetes will handle that by instantiating two additional Pods to handle more workload. You can easily check by doing kubectl get pods . Refresh your browser several times and monitor how the hostname of your microservice changes. You may need to use CTRL + F5 Kubernetes is loadbalancing your request for you ! Success Congratulations, you just learned how to scale a service in Kubernetes ! Application Config For the next step, we we'll see how to configure an application in Kubernetes. You might have noticed that in the main Java Class, we are referencing an environment variable GREETING with the default value \"You\" . In a first step, we will change the Kubernetes deployment and modify the environment variable section the Pod template: 1 kubectl edit deployment yncrea-hellomicro Then modify the GREETING variable so the \u00b5S will greet yourself, and save. The deployement should be Edited . Warning A K8S ressource edited with kubectl edit command is temporary . The modification will be lost when deleting / re-deploying the app. The modificatin needs to be reported to the file deployment.yaml after you are satisfied with the result to make it permanent at each app deployment. Refresh your browser, and see how to greeting changed. Now let's use another mean to configure our application: the Kubernetes ConfigMap . Download the sample ConfigMap to your src/kubernetes folder: configmap.yaml This way, you can decouple the application from the deployment configuration and therefore ease the reusability of your application. You can deploy the ConfigMap with the following command: 1 kubectl apply -f src/kubernetes/configmap.yaml Now, you'll have to modify your deployment in order to consume the ConfigMap: 1 kubectl edit deployment yncrea-hellomicro And edit the file in the following way: 1 2 3 4 5 6 7 8 spec : containers : - env : - name : GREETING valueFrom : configMapKeyRef : key : greeting name : yncrea-hellomicro-configmap Refresh your browser, and see how to greeting changed. Success Congratulations, you just learned how to configure an application in Kubernetes unsing Configmaps ! Application Secrets Kubernetes also supports objects of the type Secret , that are meant to store sensitive data. Secrets can either be injected as environment variables or mounted in the Pods filesystem. As you already learned how to inject environment variables, let's now inject the Kubernetes secret as a file into our pod. Deploy a secret in our Kubernetes cluster: 1 kubectl create secret generic yncrea-hellomicro-secret --from-literal=secret.txt=\"my very secret secret agent name\" Tip You can also create Secrets from a file . It will be usefull later to automatize our deployment. Update your Pod definiton (deployment) to mount the yncrea-hellomicro-secret secret in /var/secret/ and to consume it : 1 kubectl edit deployment yncrea-hellomicro And edit the file in the following way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 spec : containers : - env : - name : GREETING valueFrom : configMapKeyRef : key : greeting name : yncrea-hellomicro-configmap - name : MY_SECRET valueFrom : secretKeyRef : key : secret.txt name : yncrea-hellomicro-secret image : test/cloudcomputing:latest imagePullPolicy : IfNotPresent name : yncrea-hellomicro ports : - containerPort : 8080 name : http protocol : TCP resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /var/secret name : yncrea-hellomicro-secret readOnly : true dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - name : yncrea-hellomicro-secret secret : secretName : yncrea-hellomicro-secret Refresh your browser, and see how the /secret API changed and is now displaying your decoded secret ! Success Congratulations, you just learned how to configure an application in Kubernetes using Secrets ! Cleanup Before going to next steps, it is important to cleanup all this *** we created manually to not conflict with later all-in-one deployment with Charts. kubectl delete deployment yncrea-hellomicro kubectl delete service yncrea-hellomicro-service kubectl delete cm yncrea-hellomicro-configmap kubectl delete secret yncrea-hellomicro-secret Helm Charts To deploy our containers on kubernetes, we could create k8s objects and deploy them individually as above. But why not creating a single release of our microservice deployment, nicely packaged with all the neeeded k8s objects ? We would then be able to install, upgrade, delete ... any release of our \u00b5S, in one single command, with one single package ! The solution is : Helm Charts . Have a look to their awesome documentation ! Create the Chart According to documentation, we may adopt the appropriate structure . In our case we have a very simple deployment and we DON'T need : The charts directory since we have no subcharts The crds directory since we have no custom ressources Info You can create the Optional files (documentation mainly) if you want to. So let's proceed : Create the folders src/helm/chart/yncrea-hellomicro . Warning The last folder MUST be named according to your Chart name ! (to define in the Chart.yaml file) Inside, create the templates directory and move there the k8s objects we created previously : deployment.yaml service.yaml configmap.yaml Put the file secret.yaml in your Chart's templates directory along side other files. Info This is the secret we created manually before, now provisionned as a k8s ressource file for automation purpose ! Edit the deployment.yaml file (if not done already) to report the changes done previously (when we used kubectl edit commands) to get the env variables from the Configmap and the Secret. In the Chart folder ( src/helm/chart/yncrea-hellomicro ), download and put : Chart.yaml values.yaml Info Chart.yaml is the declaration of your Chart values.yaml is the file meant to configure your Chart Use the values defined in values.yaml to configure your Chart. Solution for deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : apps/v1 kind : Deployment metadata : name : yncrea-hellomicro labels : app.kubernetes.io/name : yncrea-hellomicro spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : app.kubernetes.io/name : yncrea-hellomicro template : metadata : labels : app.kubernetes.io/name : yncrea-hellomicro spec : containers : - name : yncrea-hellomicro image : {{ .Values.image.repository }} :{{ .Values.image.version }} imagePullPolicy : IfNotPresent ports : - name : http containerPort : 8080 protocol : TCP Solution for service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : yncrea-hellomicro-service spec : selector : app.kubernetes.io/name : yncrea-hellomicro ports : - protocol : TCP port : 80 targetPort : 8080 nodePort : {{ .Values.service.nodePort }} type : {{ .Values.service.type }} Deploy the Chart to Minikube using Helm Simply use the Helm CLI to deploy the Chart : 1 helm install silly-unicorn src/helm/chart/yncrea-hellomicro Note We fix the name of the release ( silly-unicorn ) in order to be able to upgrade/delete it automatically. Otherwise, Helm would assign a random name. Verify your Chart deployed properly the \u00b5S (its deployment , pods , configmap , secret and service ). Check everything is working as expected by accessing your REST APIs ( / , /hello , /secret ) in your browser as done before. Success Congratulation, you deployed your fisrt Chart (or Application) using Helm ! Upgrade the release At this moment, we have our Chart ready, our app is running. But what if we want to update it ? Let's do a change in the code and add a new API /new returning a String \"Hello from New API\" Rebuild your project to publish your new release. Then try upgrading your application using : helm upgrade silly-unicorn src/helm/chart/yncrea-hellomicro Check the /new API is there and returning expected value. Success Congratulation, you upgraded your first application using Helm ! Deploying automatically with Maven : Automatize the build using Maven Helm plugin : Find the latest Helm plugin online Add the dependency to you pom. Solution <dependency> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> </dependency> Configure the plugin to : Verify the Chart in the test phase using the lint goal Package the Chart into an archive in the package phase, using the package goal. Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 <build> <plugins> ... <plugin> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> <configuration> <chartDirectory> ${project.basedir}/src/helm/chart/yncrea-hellomicro </chartDirectory> <chartVersion> ${project.version} </chartVersion> <outputDirectory> ${project.build.directory} </outputDirectory> <!-- This is the related section to use local binary with auto-detection enabled. --> <useLocalHelmBinary> true </useLocalHelmBinary> </configuration> <executions> <execution> <id> helm-lint </id> <phase> test </phase> <goals> <goal> lint </goal> </goals> </execution> <execution> <id> helm-package </id> <phase> package </phase> <goals> <goal> package </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Finally use a third plugin : exec-maven-plugin in order to (in the maven pre-integration-test phase): Delete the fixed-name release silly-unicorn if it exists. Deploy the new release with the same name, of the Chart we just built Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 <build> <plugins> ... <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <version> ${exec-maven-plugin.version} </version> <executions> <execution> <id> exec-helm-delete </id> <phase> pre-integration-test </phase> <configuration> <executable> helm </executable> <commandlineArgs> delete silly-unicorn --purge </commandlineArgs> <successCodes> <successCode> 0 </successCode> <successCode> 1 </successCode> </successCodes> </configuration> <goals> <goal> exec </goal> </goals> </execution> <execution> <id> exec-helm-install </id> <phase> pre-integration-test </phase> <configuration> <executable> helm </executable> <commandlineArgs> install ${project.build.directory}/${project.artifactId}-${project.version}.tgz --name silly-unicorn --wait --debug </commandlineArgs> </configuration> <goals> <goal> exec </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Last step, test the whole Docker + Helm chart build and deploy using maven ! mvn clean install It will create the docker image, push it to minikube registry. Then it will build the Helm chart, and install it to minikube, using the just pushed docker image. Access your application REST APIs with your browser. Example : http://192.168.99.100:30080/hello Success Congratulation, you can now do any code change on the application java code or the chart config, you just have to execute a mvn clean install command to build and redeploy your latest app version automatically ! Remote Debug In order to develop efficiently, remote debug is a must have. First we need to update our Dockerfile to bootstrap a remote debug port (8000). FROM java:8 EXPOSE 8080 ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-agentlib:jdwp=transport=dt_socket,address=8000,server=y,suspend=n\" , \"-jar\" , \"/app.jar\" ] Warning Don't forget to rebuild your Docker Image and to push it again to your local registry (or DockerHub). Then we need to expose this port by modifying the deployment.yaml ... ports : - name : http containerPort : 8080 protocol : TCP - name : remote-debug containerPort : 8000 protocol : TCP And finally create a NodePort in the service.yaml to make the debug port available from outside the K8S Cluster Tip A good practice is to make this NodePort configurable through the values.yaml #values.yaml service : type : NodePort nodePort : 30080 debugNodePort : 30085 #service.yaml ports : - name : http protocol : TCP port : 80 targetPort : 8080 nodePort : {{ .Values.service.nodePort }} - name : remote-debug protocol : TCP port : 8000 targetPort : 8000 nodePort : {{ .Values.service.debugNodePort }} We can now update your Chart with the latest docker image and changes. Let's create a remote debug configuration in IntelliJ Idea, targetting our remote debug NodePort : Try run this remote debug configuration, place a breakpoint in the Java Code of the /hello API Method. Call the REST API using your browser and observe the execution stopping at your breakpoint. Success Congratulation, you can now deploy Helm Charts in Kubenretes & remote debug your Java Applications !","title":"Kubernetes - Charts Lab"},{"location":"kubernetes/#kubernetes-lab","text":"We saw that Docker helps creating Containers. Now we need to manage these containers : here comes Kubernetes, the \"Container Orchestrator\" , also called \"K8S\" . Please visit their website explaining in details what is K8S ... In this lab, we will use Minikube as a local kubernetes, for develoment only. Info No matter if you used Gokube for previous Labs, it is running Minikube behind the scenes If you used Docker Desktop you don't have Minikube but an embedded K8S Cluster working the same way, all good !","title":"Kubernetes Lab"},{"location":"kubernetes/#simple-application-deployment","text":"Verify that you can access Kubernetes: 1 kubectl version If you see a Server Version like below, it means your Kubernetes CLI can connect to your Kubernetes VM: 1 2 3 $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.10\" , GitCommit: \"37d169313237cb4ceb2cc4bef300f2ae3053c1a2\" , GitTreeState: \"clean\" , BuildDate: \"2019-08-19T10:52:43Z\" , GoVersion: \"go1.11.13\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.10\" , GitCommit: \"37d169313237cb4ceb2cc4bef300f2ae3053c1a2\" , GitTreeState: \"clean\" , BuildDate: \"2019-08-19T10:44:49Z\" , GoVersion: \"go1.11.13\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Create the folder src/kubernetes in your project. Download the following file to your the newly created folder: deployment.yaml Open the file in an editor and verify that the image: key is referencing your previously built image. Deploy your application with the following command: 1 kubectl apply -f src/kubernetes/deployment.yaml Verify that your application is running properly: 1 kubectl get deployment You shoud now see one running Pod , which is scheduled by the Deployment that you just created. Tip The number of pods (more specifically, the number of containers) running is displayed in the \"READY\" column. 1/1 means 1 container is working fine out of 1 expected, all good ! Info Even if you have one pod running, if now try to access your application REST APIs through your browser, it should NOT work. (Unless you forgot to delete the docker container from previous Lab, in this case, the old container is repsonding, not the newly created pod !) You can also check the running Pods in your Kubernetes cluster by typing: 1 kubectl get pods This will give a list of running instances (a.k.a. Pods) of your application. Tip Write down the name of the Pod, you'll need it later for reference. In order to access your application, you have to deploy a Kubernetes Service . Download the following file to your kubernetes folder: service.yaml and apply the following command: 1 kubectl apply -f src/kubernetes/service.yaml You have now deployed a so called NodePort Kubernetes Service . It opens a dedicated port on your Minikube VM, through which you can access the according service. You can find the associated port number by typing: 1 kubectl get svc In the example below, the port number would be 30080 : 1 2 3 4 5 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 14d yncrea-hellomicro-service NodePort 10 .103.108.153 <none> 80 :30080/TCP 2m15s Info If not set in the service, Kubernetes would allow a random NodePort between 30000 and 32767. In your browser, open the IP of your Minikube VM (which you retrieved in the docker lab) and add the port that you retrieved from the last command, e.g.: http://minikube-ip:30080 . Tip When using Docker Desktop embedded K8S in place of minikube, you can directly hit on http://localhost:30080 ! Success You should see \"Hello World\" example from before, but it's hosted in Kubernetes . In addition you will notice that the micro service is now displaying the hostname . Indeed, now the container is orchestrated by kubernetes, and kubernetes filled the default env vars such as HOSTNAME used in our microservice Java Code ;)","title":"Simple Application Deployment"},{"location":"kubernetes/#application-scaling","text":"Now you'll see the scaling capabilities of Kubernetes. Enter the following command: 1 kubectl.exe scale deployment/yncrea-hellomicro --replicas=3 With this command, you update the Kubernetes Deployment and instruct it to have a total of three replicas. Kubernetes will handle that by instantiating two additional Pods to handle more workload. You can easily check by doing kubectl get pods . Refresh your browser several times and monitor how the hostname of your microservice changes. You may need to use CTRL + F5 Kubernetes is loadbalancing your request for you ! Success Congratulations, you just learned how to scale a service in Kubernetes !","title":"Application Scaling"},{"location":"kubernetes/#application-config","text":"For the next step, we we'll see how to configure an application in Kubernetes. You might have noticed that in the main Java Class, we are referencing an environment variable GREETING with the default value \"You\" . In a first step, we will change the Kubernetes deployment and modify the environment variable section the Pod template: 1 kubectl edit deployment yncrea-hellomicro Then modify the GREETING variable so the \u00b5S will greet yourself, and save. The deployement should be Edited . Warning A K8S ressource edited with kubectl edit command is temporary . The modification will be lost when deleting / re-deploying the app. The modificatin needs to be reported to the file deployment.yaml after you are satisfied with the result to make it permanent at each app deployment. Refresh your browser, and see how to greeting changed. Now let's use another mean to configure our application: the Kubernetes ConfigMap . Download the sample ConfigMap to your src/kubernetes folder: configmap.yaml This way, you can decouple the application from the deployment configuration and therefore ease the reusability of your application. You can deploy the ConfigMap with the following command: 1 kubectl apply -f src/kubernetes/configmap.yaml Now, you'll have to modify your deployment in order to consume the ConfigMap: 1 kubectl edit deployment yncrea-hellomicro And edit the file in the following way: 1 2 3 4 5 6 7 8 spec : containers : - env : - name : GREETING valueFrom : configMapKeyRef : key : greeting name : yncrea-hellomicro-configmap Refresh your browser, and see how to greeting changed. Success Congratulations, you just learned how to configure an application in Kubernetes unsing Configmaps !","title":"Application Config"},{"location":"kubernetes/#application-secrets","text":"Kubernetes also supports objects of the type Secret , that are meant to store sensitive data. Secrets can either be injected as environment variables or mounted in the Pods filesystem. As you already learned how to inject environment variables, let's now inject the Kubernetes secret as a file into our pod. Deploy a secret in our Kubernetes cluster: 1 kubectl create secret generic yncrea-hellomicro-secret --from-literal=secret.txt=\"my very secret secret agent name\" Tip You can also create Secrets from a file . It will be usefull later to automatize our deployment. Update your Pod definiton (deployment) to mount the yncrea-hellomicro-secret secret in /var/secret/ and to consume it : 1 kubectl edit deployment yncrea-hellomicro And edit the file in the following way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 spec : containers : - env : - name : GREETING valueFrom : configMapKeyRef : key : greeting name : yncrea-hellomicro-configmap - name : MY_SECRET valueFrom : secretKeyRef : key : secret.txt name : yncrea-hellomicro-secret image : test/cloudcomputing:latest imagePullPolicy : IfNotPresent name : yncrea-hellomicro ports : - containerPort : 8080 name : http protocol : TCP resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /var/secret name : yncrea-hellomicro-secret readOnly : true dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - name : yncrea-hellomicro-secret secret : secretName : yncrea-hellomicro-secret Refresh your browser, and see how the /secret API changed and is now displaying your decoded secret ! Success Congratulations, you just learned how to configure an application in Kubernetes using Secrets !","title":"Application Secrets"},{"location":"kubernetes/#cleanup","text":"Before going to next steps, it is important to cleanup all this *** we created manually to not conflict with later all-in-one deployment with Charts. kubectl delete deployment yncrea-hellomicro kubectl delete service yncrea-hellomicro-service kubectl delete cm yncrea-hellomicro-configmap kubectl delete secret yncrea-hellomicro-secret","title":"Cleanup"},{"location":"kubernetes/#helm-charts","text":"To deploy our containers on kubernetes, we could create k8s objects and deploy them individually as above. But why not creating a single release of our microservice deployment, nicely packaged with all the neeeded k8s objects ? We would then be able to install, upgrade, delete ... any release of our \u00b5S, in one single command, with one single package ! The solution is : Helm Charts . Have a look to their awesome documentation ! Create the Chart According to documentation, we may adopt the appropriate structure . In our case we have a very simple deployment and we DON'T need : The charts directory since we have no subcharts The crds directory since we have no custom ressources Info You can create the Optional files (documentation mainly) if you want to. So let's proceed : Create the folders src/helm/chart/yncrea-hellomicro . Warning The last folder MUST be named according to your Chart name ! (to define in the Chart.yaml file) Inside, create the templates directory and move there the k8s objects we created previously : deployment.yaml service.yaml configmap.yaml Put the file secret.yaml in your Chart's templates directory along side other files. Info This is the secret we created manually before, now provisionned as a k8s ressource file for automation purpose ! Edit the deployment.yaml file (if not done already) to report the changes done previously (when we used kubectl edit commands) to get the env variables from the Configmap and the Secret. In the Chart folder ( src/helm/chart/yncrea-hellomicro ), download and put : Chart.yaml values.yaml Info Chart.yaml is the declaration of your Chart values.yaml is the file meant to configure your Chart Use the values defined in values.yaml to configure your Chart. Solution for deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : apps/v1 kind : Deployment metadata : name : yncrea-hellomicro labels : app.kubernetes.io/name : yncrea-hellomicro spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : app.kubernetes.io/name : yncrea-hellomicro template : metadata : labels : app.kubernetes.io/name : yncrea-hellomicro spec : containers : - name : yncrea-hellomicro image : {{ .Values.image.repository }} :{{ .Values.image.version }} imagePullPolicy : IfNotPresent ports : - name : http containerPort : 8080 protocol : TCP Solution for service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : yncrea-hellomicro-service spec : selector : app.kubernetes.io/name : yncrea-hellomicro ports : - protocol : TCP port : 80 targetPort : 8080 nodePort : {{ .Values.service.nodePort }} type : {{ .Values.service.type }} Deploy the Chart to Minikube using Helm Simply use the Helm CLI to deploy the Chart : 1 helm install silly-unicorn src/helm/chart/yncrea-hellomicro Note We fix the name of the release ( silly-unicorn ) in order to be able to upgrade/delete it automatically. Otherwise, Helm would assign a random name. Verify your Chart deployed properly the \u00b5S (its deployment , pods , configmap , secret and service ). Check everything is working as expected by accessing your REST APIs ( / , /hello , /secret ) in your browser as done before. Success Congratulation, you deployed your fisrt Chart (or Application) using Helm ! Upgrade the release At this moment, we have our Chart ready, our app is running. But what if we want to update it ? Let's do a change in the code and add a new API /new returning a String \"Hello from New API\" Rebuild your project to publish your new release. Then try upgrading your application using : helm upgrade silly-unicorn src/helm/chart/yncrea-hellomicro Check the /new API is there and returning expected value. Success Congratulation, you upgraded your first application using Helm !","title":"Helm Charts"},{"location":"kubernetes/#deploying-automatically-with-maven","text":"Automatize the build using Maven Helm plugin : Find the latest Helm plugin online Add the dependency to you pom. Solution <dependency> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> </dependency> Configure the plugin to : Verify the Chart in the test phase using the lint goal Package the Chart into an archive in the package phase, using the package goal. Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 <build> <plugins> ... <plugin> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> <configuration> <chartDirectory> ${project.basedir}/src/helm/chart/yncrea-hellomicro </chartDirectory> <chartVersion> ${project.version} </chartVersion> <outputDirectory> ${project.build.directory} </outputDirectory> <!-- This is the related section to use local binary with auto-detection enabled. --> <useLocalHelmBinary> true </useLocalHelmBinary> </configuration> <executions> <execution> <id> helm-lint </id> <phase> test </phase> <goals> <goal> lint </goal> </goals> </execution> <execution> <id> helm-package </id> <phase> package </phase> <goals> <goal> package </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Finally use a third plugin : exec-maven-plugin in order to (in the maven pre-integration-test phase): Delete the fixed-name release silly-unicorn if it exists. Deploy the new release with the same name, of the Chart we just built Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 <build> <plugins> ... <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <version> ${exec-maven-plugin.version} </version> <executions> <execution> <id> exec-helm-delete </id> <phase> pre-integration-test </phase> <configuration> <executable> helm </executable> <commandlineArgs> delete silly-unicorn --purge </commandlineArgs> <successCodes> <successCode> 0 </successCode> <successCode> 1 </successCode> </successCodes> </configuration> <goals> <goal> exec </goal> </goals> </execution> <execution> <id> exec-helm-install </id> <phase> pre-integration-test </phase> <configuration> <executable> helm </executable> <commandlineArgs> install ${project.build.directory}/${project.artifactId}-${project.version}.tgz --name silly-unicorn --wait --debug </commandlineArgs> </configuration> <goals> <goal> exec </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Last step, test the whole Docker + Helm chart build and deploy using maven ! mvn clean install It will create the docker image, push it to minikube registry. Then it will build the Helm chart, and install it to minikube, using the just pushed docker image. Access your application REST APIs with your browser. Example : http://192.168.99.100:30080/hello Success Congratulation, you can now do any code change on the application java code or the chart config, you just have to execute a mvn clean install command to build and redeploy your latest app version automatically !","title":"Deploying automatically with Maven :"},{"location":"kubernetes/#remote-debug","text":"In order to develop efficiently, remote debug is a must have. First we need to update our Dockerfile to bootstrap a remote debug port (8000). FROM java:8 EXPOSE 8080 ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-agentlib:jdwp=transport=dt_socket,address=8000,server=y,suspend=n\" , \"-jar\" , \"/app.jar\" ] Warning Don't forget to rebuild your Docker Image and to push it again to your local registry (or DockerHub). Then we need to expose this port by modifying the deployment.yaml ... ports : - name : http containerPort : 8080 protocol : TCP - name : remote-debug containerPort : 8000 protocol : TCP And finally create a NodePort in the service.yaml to make the debug port available from outside the K8S Cluster Tip A good practice is to make this NodePort configurable through the values.yaml #values.yaml service : type : NodePort nodePort : 30080 debugNodePort : 30085 #service.yaml ports : - name : http protocol : TCP port : 80 targetPort : 8080 nodePort : {{ .Values.service.nodePort }} - name : remote-debug protocol : TCP port : 8000 targetPort : 8000 nodePort : {{ .Values.service.debugNodePort }} We can now update your Chart with the latest docker image and changes. Let's create a remote debug configuration in IntelliJ Idea, targetting our remote debug NodePort : Try run this remote debug configuration, place a breakpoint in the Java Code of the /hello API Method. Call the REST API using your browser and observe the execution stopping at your breakpoint. Success Congratulation, you can now deploy Helm Charts in Kubenretes & remote debug your Java Applications !","title":"Remote Debug"},{"location":"maven/","text":"Getting Started with Maven Maven provides a simple and fast way to build and deploy applications. In this course we will use it to automate our development phases. Install Maven Well, if you are using a decent IDE, it is already embedded Tip You may need to add it to your PATH to use outside your IDE ... Otherwise, you can install it using chocolatey : 1 choco install maven The POM This is where all the magic happens ! It is the xml file where you can add all the things your app needs to work. Dependencies: The libraries your application need to function. These are called dependencies . 1 2 3 4 5 6 7 8 <dependencies> ... <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> ... </dependencies> Plugins Maven is - at its heart - a plugin execution framework. All work is done by plugins . The best idea is to look online for a plugin doing what you want to do. In our case, we want to build Docker images and create Helm Charts for Kubernetes. Fortunately, numerous plugins doing such thing are available. We may use : Fabric8 Plugin Helm Plugin They are declared in the pom as following : Warning This is just an example, DO NOT change your code right now :) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <build> ... <plugin> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> <configuration> ... </configuration> <executions> <execution> <id> helm-package </id> <phase> package </phase> <goals> <goal> package </goal> </goals> </execution> </executions> </plugin> ... </build> One important thing to note is that we can configure plugins. We can also set when they enter the game in the build by associating them with maven phases . And of course, tell the plugin what to do using its own goals . Tip Maven has several phases available, happening in this order : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <phases> <phase> validate </phase> <phase> initialize </phase> <phase> generate-sources </phase> <phase> process-sources </phase> <phase> generate-resources </phase> <phase> process-resources </phase> <phase> compile </phase> <phase> process-classes </phase> <phase> generate-test-sources </phase> <phase> process-test-sources </phase> <phase> generate-test-resources </phase> <phase> process-test-resources </phase> <phase> test-compile </phase> <phase> process-test-classes </phase> <phase> test </phase> <phase> prepare-package </phase> <phase> package </phase> <phase> pre-integration-test </phase> <phase> integration-test </phase> <phase> post-integration-test </phase> <phase> verify </phase> <phase> install </phase> <phase> deploy </phase> </phases>","title":"Code - Get Started with Maven"},{"location":"maven/#getting-started-with-maven","text":"Maven provides a simple and fast way to build and deploy applications. In this course we will use it to automate our development phases. Install Maven Well, if you are using a decent IDE, it is already embedded Tip You may need to add it to your PATH to use outside your IDE ... Otherwise, you can install it using chocolatey : 1 choco install maven The POM This is where all the magic happens ! It is the xml file where you can add all the things your app needs to work. Dependencies: The libraries your application need to function. These are called dependencies . 1 2 3 4 5 6 7 8 <dependencies> ... <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> ... </dependencies> Plugins Maven is - at its heart - a plugin execution framework. All work is done by plugins . The best idea is to look online for a plugin doing what you want to do. In our case, we want to build Docker images and create Helm Charts for Kubernetes. Fortunately, numerous plugins doing such thing are available. We may use : Fabric8 Plugin Helm Plugin They are declared in the pom as following : Warning This is just an example, DO NOT change your code right now :) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <build> ... <plugin> <groupId> com.kiwigrid </groupId> <artifactId> helm-maven-plugin </artifactId> <version> 5.4 </version> <configuration> ... </configuration> <executions> <execution> <id> helm-package </id> <phase> package </phase> <goals> <goal> package </goal> </goals> </execution> </executions> </plugin> ... </build> One important thing to note is that we can configure plugins. We can also set when they enter the game in the build by associating them with maven phases . And of course, tell the plugin what to do using its own goals . Tip Maven has several phases available, happening in this order : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <phases> <phase> validate </phase> <phase> initialize </phase> <phase> generate-sources </phase> <phase> process-sources </phase> <phase> generate-resources </phase> <phase> process-resources </phase> <phase> compile </phase> <phase> process-classes </phase> <phase> generate-test-sources </phase> <phase> process-test-sources </phase> <phase> generate-test-resources </phase> <phase> process-test-resources </phase> <phase> test-compile </phase> <phase> process-test-classes </phase> <phase> test </phase> <phase> prepare-package </phase> <phase> package </phase> <phase> pre-integration-test </phase> <phase> integration-test </phase> <phase> post-integration-test </phase> <phase> verify </phase> <phase> install </phase> <phase> deploy </phase> </phases>","title":"Getting Started with Maven"},{"location":"prerequisites/","text":"Prerequisites Please follow the instructions on this page carefully , as they will help you avoiding obstacles in the next exercices. The goal of the prerequisite step is to provide you a fully working development environment, containing Docker and Kubernetes. Tip If you are using Windows 10 or Mac, you can also use Docker Desktop in place of minikube. In order to achieve that, you'll be guided through the following steps: Create a Github Account if you don't already have one Create a Docker Hub Account if you don't already have one Install Chocolatey, a Package Manager for Windows With Chocolatey, you will install the following packages on your workstation: VirtualBox, a VM Manager Docker CLI and Kubernetes CLI Minikube, a tool that helps you installing a Docker and Kubernetes Development environment AWS CLI, you'll need it later in the course With Minikube, you will install a Virtual Machine in VirtualBox, containing Docker and Kubernetes The diagram on the bottom of this page is designed to help you to understand how Windows, your VM, Docker and Kubernetes are interacting. To perform this lab: Install Chocolatey: According to the instructions here . You do not have to enter your email address in the first step. Install needed Tools: Launch a terminal (CMD, PowerShell or GitBash etc ...) with Windows Administrator rights and install VirtualBox, Python, Minikube, Kubectl (the Kubernetes CLI), Helm and the Docker CLI with the help of Chocolatey: Warning Please remove the items you arleady have in the command below, no need to reinstall them. Install Docker using Chocolatey according to your Windows Version : Windows 10 with Hyper-V : choco install -y python awscli docker-desktop kubernetes-helm Windows 7, 8, 10 Without Hyper-V : Use Gokube choco install -y python awscli virtualbox kubernetes-helm gokube init Mac OS : Install Docker Desktop and enable Kubernetes Linux : Install manually Docker + Helm + Kubernetes Cli + AWS Cli + Python Launch Gokube or Minikube depending on your case (If not using Docker Desktop): Warning If your are not using ISEN Network, DO NOT set the proxy settings. minikube --docker-env HTTP_PROXY = \"http://<isen-proxy-host>:<isen-proxy-port>\" --docker-env HTTPS_PROXY = \"http://<isen-proxy-host>:<isen-proxy-port>\" --docker-env NO_PROXY = \"127.0.0.1,192.168.99.0/24,10.0.0.0/8\" start or gokube init --http-proxy \"http://<isen-proxy-host>:<isen-proxy-port>\" --https-proxy \"http://<isen-proxy-host>:<isen-proxy-port>\" --no-proxy \"127.0.0.1,192.168.99.0/24,10.0.0.0/8\" Info As we are in a universtiy network, we need to configure docker engine in the virtual machine to perform outgoing internet connection through this proxy. Certain hosts do not need to be accessed through the proxy, which is configured through the \"NO_PROXY\" parameter. In our case, this is: localhost (127.0.0.1) the network between your local VMs (192.168.99.0/24), the network range normally used for intranet (10.0.0.0/8) In order for Minikube to download the according VM image, you may have to configure the proxy on your workstation as well: For Windows Terminal: 1 2 3 4 #!/usr/bin/bash set HTTP_PROXY = http://<isen-proxy-host>:<isen-proxy-port> set HTTPS_PROXY = http://<isen-proxy-host>:<isen-proxy-port> set NO_PROXY = 127 .0.0.1,192.168.99.0/24,10.0.0.0/8 For Shell (Cygwin, Git Bash): 1 2 3 4 #!/usr/bin/bash export HTTP_PROXY = http://<isen-proxy-host>:<isen-proxy-port> export HTTPS_PROXY = http://<isen-proxy-host>:<isen-proxy-port> export NO_PROXY = 127 .0.0.1,192.168.99.0/24,10.0.0.0/8 You can now take a waiting for everything to be ready ! Configure CLI (If NOT using Docker Desktop): After Minikube is launched, it can be necessary to configure your Docker CLI and Kubernetes CLI on your workstation. This configuration is done through environment variables, which can be set with the following commands : Danger This has to be done every time you open a new terminal For Windows Terminal: 1 @FOR /f \"tokens=*\" %i IN ('minikube docker-env') DO @%i For Shell (Cygwin, Git Bash): eval ( minikube docker-env ) or eval $( minikube docker-env ) It will set following variables : export DOCKER_TLS_VERIFY = \"1\" export DOCKER_HOST = \"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH = \"/Users/jberger/.minikube/certs\" export MINIKUBE_ACTIVE_DOCKERD = \"minikube\" This step allows Docker CLI and Kubernetes CLI to target the daemons inside the Minikube VM. If you don't do this, your docker image will be pushed to your local Docker Registry, and the Kubernetes Cluster on the Minikube VM won't be able to access it ! To go back to the default config if needed, you can run : eval \" $( docker-machine env -u ) \" Verify that you can access the Docker CLI: 1 docker info Verify that you can access the Kubernetes CLI: 1 kubectl version Once satisfied with your configuration, make it permanent and system wide : On Shell by editing your .bashrc file and sourcing it On Windows by settings the environement variables manually. Tip While using an IDE, you may have to restart it for these changes to take effect ... Info Again, to go back to default config, you'll have to unset the variables (or remove them on Windows) : unset DOCKER_TLS_VERIFY unset DOCKER_HOST unset DOCKER_CERT_PATH unset DOCKER_MACHINE_NAME Below you can see a diagram of your workstation setup, which should help you understanding how the different components are interacting:","title":"Env - Prerequisites"},{"location":"prerequisites/#prerequisites","text":"Please follow the instructions on this page carefully , as they will help you avoiding obstacles in the next exercices. The goal of the prerequisite step is to provide you a fully working development environment, containing Docker and Kubernetes. Tip If you are using Windows 10 or Mac, you can also use Docker Desktop in place of minikube. In order to achieve that, you'll be guided through the following steps: Create a Github Account if you don't already have one Create a Docker Hub Account if you don't already have one Install Chocolatey, a Package Manager for Windows With Chocolatey, you will install the following packages on your workstation: VirtualBox, a VM Manager Docker CLI and Kubernetes CLI Minikube, a tool that helps you installing a Docker and Kubernetes Development environment AWS CLI, you'll need it later in the course With Minikube, you will install a Virtual Machine in VirtualBox, containing Docker and Kubernetes The diagram on the bottom of this page is designed to help you to understand how Windows, your VM, Docker and Kubernetes are interacting. To perform this lab: Install Chocolatey: According to the instructions here . You do not have to enter your email address in the first step. Install needed Tools: Launch a terminal (CMD, PowerShell or GitBash etc ...) with Windows Administrator rights and install VirtualBox, Python, Minikube, Kubectl (the Kubernetes CLI), Helm and the Docker CLI with the help of Chocolatey: Warning Please remove the items you arleady have in the command below, no need to reinstall them. Install Docker using Chocolatey according to your Windows Version : Windows 10 with Hyper-V : choco install -y python awscli docker-desktop kubernetes-helm Windows 7, 8, 10 Without Hyper-V : Use Gokube choco install -y python awscli virtualbox kubernetes-helm gokube init Mac OS : Install Docker Desktop and enable Kubernetes Linux : Install manually Docker + Helm + Kubernetes Cli + AWS Cli + Python Launch Gokube or Minikube depending on your case (If not using Docker Desktop): Warning If your are not using ISEN Network, DO NOT set the proxy settings. minikube --docker-env HTTP_PROXY = \"http://<isen-proxy-host>:<isen-proxy-port>\" --docker-env HTTPS_PROXY = \"http://<isen-proxy-host>:<isen-proxy-port>\" --docker-env NO_PROXY = \"127.0.0.1,192.168.99.0/24,10.0.0.0/8\" start or gokube init --http-proxy \"http://<isen-proxy-host>:<isen-proxy-port>\" --https-proxy \"http://<isen-proxy-host>:<isen-proxy-port>\" --no-proxy \"127.0.0.1,192.168.99.0/24,10.0.0.0/8\" Info As we are in a universtiy network, we need to configure docker engine in the virtual machine to perform outgoing internet connection through this proxy. Certain hosts do not need to be accessed through the proxy, which is configured through the \"NO_PROXY\" parameter. In our case, this is: localhost (127.0.0.1) the network between your local VMs (192.168.99.0/24), the network range normally used for intranet (10.0.0.0/8) In order for Minikube to download the according VM image, you may have to configure the proxy on your workstation as well: For Windows Terminal: 1 2 3 4 #!/usr/bin/bash set HTTP_PROXY = http://<isen-proxy-host>:<isen-proxy-port> set HTTPS_PROXY = http://<isen-proxy-host>:<isen-proxy-port> set NO_PROXY = 127 .0.0.1,192.168.99.0/24,10.0.0.0/8 For Shell (Cygwin, Git Bash): 1 2 3 4 #!/usr/bin/bash export HTTP_PROXY = http://<isen-proxy-host>:<isen-proxy-port> export HTTPS_PROXY = http://<isen-proxy-host>:<isen-proxy-port> export NO_PROXY = 127 .0.0.1,192.168.99.0/24,10.0.0.0/8 You can now take a waiting for everything to be ready ! Configure CLI (If NOT using Docker Desktop): After Minikube is launched, it can be necessary to configure your Docker CLI and Kubernetes CLI on your workstation. This configuration is done through environment variables, which can be set with the following commands : Danger This has to be done every time you open a new terminal For Windows Terminal: 1 @FOR /f \"tokens=*\" %i IN ('minikube docker-env') DO @%i For Shell (Cygwin, Git Bash): eval ( minikube docker-env ) or eval $( minikube docker-env ) It will set following variables : export DOCKER_TLS_VERIFY = \"1\" export DOCKER_HOST = \"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH = \"/Users/jberger/.minikube/certs\" export MINIKUBE_ACTIVE_DOCKERD = \"minikube\" This step allows Docker CLI and Kubernetes CLI to target the daemons inside the Minikube VM. If you don't do this, your docker image will be pushed to your local Docker Registry, and the Kubernetes Cluster on the Minikube VM won't be able to access it ! To go back to the default config if needed, you can run : eval \" $( docker-machine env -u ) \" Verify that you can access the Docker CLI: 1 docker info Verify that you can access the Kubernetes CLI: 1 kubectl version Once satisfied with your configuration, make it permanent and system wide : On Shell by editing your .bashrc file and sourcing it On Windows by settings the environement variables manually. Tip While using an IDE, you may have to restart it for these changes to take effect ... Info Again, to go back to default config, you'll have to unset the variables (or remove them on Windows) : unset DOCKER_TLS_VERIFY unset DOCKER_HOST unset DOCKER_CERT_PATH unset DOCKER_MACHINE_NAME Below you can see a diagram of your workstation setup, which should help you understanding how the different components are interacting:","title":"Prerequisites"}]}